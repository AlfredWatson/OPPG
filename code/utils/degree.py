import openai
from openai import OpenAI
import  os
import httpx
import backoff
import time
import json
import random
from openai import RateLimitError, APIError, APIConnectionError
# from .openai_utils import OutOfQuotaException, AccessTerminatedException
# from .openai_utils import num_tokens_from_string, model2max_context
import ollama
import requests


class Agent:
    def __init__(self, model_name: str, name: str, temperature: float, sleep_time: float=0) -> None:
        """Create an agent

        Args:
            model_name(str): model name
            name (str): name of this agent
            temperature (float): higher values make the output more random, while lower values make it more focused and deterministic
            sleep_time (float): sleep because of rate limits
        """
        self.model_name = model_name
        self.name = name
        self.temperature = temperature
        self.memory_lst = []
        self.sleep_time = sleep_time

    @backoff.on_exception(backoff.expo, (RateLimitError, APIError, APIConnectionError), max_tries=20)
    def query(self, messages: "list[dict]", api_key: str, temperature: float) -> str:
        """make a query

        Args:
            messages (list[dict]): chat history in turbo format
            max_tokens (int): max token in api call
            api_key (str): openai api key
            temperature (float): sampling temperature

        Raises:
            OutOfQuotaException: the apikey has out of quota
            AccessTerminatedException: the apikey has been ban

        Returns:
            str: the return msg
        """
        time.sleep(self.sleep_time)
        response = ollama.chat(
            model='qwen2.5:72b',  # 这里使用你指定的模型名称
            messages=messages,

        )
        # 获取响应中的内容
        gen = response['message']['content']
        return gen  # 返回响应的内容


    def set_meta_prompt(self, meta_prompt: str):
        """Set the meta_prompt

        Args:
            meta_prompt (str): the meta prompt
        """
        self.memory_lst.append({"role": "system", "content": f"{meta_prompt}"})

    def add_event(self, event: str):
        """Add an new event in the memory

        Args:
            event (str): string that describe the event.
        """
        self.memory_lst.append({"role": "user", "content": f"{event}"})

    def add_memory(self, memory: str):
        """Monologue in the memory

        Args:
            memory (str): string that generated by the model in the last round.
        """
        self.memory_lst.append({"role": "assistant", "content": f"{memory}"})
        print(f"----- {self.name} -----\n{memory}\n")

    def ask_grade1(self, text, temperature: float = None):
        """Query for answer

        Args:
        """
        url = "http://localhost:8000/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "gpt-3.5-turbo",  
            "messages": [
                {"role": "system", "content": ""},
                {"role": "user", "content": text}
            ], 
            "max_tokens": 512,
            "temperature": temperature
        }

        response = requests.post(url, json=data, headers=headers)
        response_json = response.json()
        assistant_content = response_json.get("choices", [])[0].get("message", {}).get("content", "")
        print(assistant_content)

        return assistant_content
    
    def ask_grade2(self, text, temperature: float = None):
        """Query for answer

        Args:
        """
        # query

        url = "http://localhost:8001/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "gpt-3.5-turbo",  
            "messages": [
                {"role": "system", "content": ""},
                {"role": "user", "content": text}
            ], 
            "max_tokens": 512,
            "temperature": temperature
        }

        response = requests.post(url, json=data, headers=headers)
        response_json = response.json()
        assistant_content = response_json.get("choices", [])[0].get("message", {}).get("content", "")
        print(assistant_content)

        return assistant_content
    

    def ask_grade3(self, text, temperature: float = None):
        """Query for answer

        Args:
        """
        # query

        url = "http://localhost:8002/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "gpt-3.5-turbo",  
            "messages": [
                {"role": "system", "content": ""},
                {"role": "user", "content": text}
            ], 
            "max_tokens": 512,
            "temperature": temperature
        }

        response = requests.post(url, json=data, headers=headers)
        response_json = response.json()
        assistant_content = response_json.get("choices", [])[0].get("message", {}).get("content", "")
        print(assistant_content)

        return assistant_content
    
    def ask_grade4(self, text, temperature: float = None):
        """Query for answer

        Args:
        """
        # query

        url = "http://localhost:8003/v1/chat/completions"
        headers = {"Content-Type": "application/json"}
        data = {
            "model": "gpt-3.5-turbo",  
            "messages": [
                {"role": "system", "content": ""},
                {"role": "user", "content": text}
            ], 
            "max_tokens": 512,
            "temperature":temperature
        }

        response = requests.post(url, json=data, headers=headers)
        response_json = response.json()
        assistant_content = response_json.get("choices", [])[0].get("message", {}).get("content", "")
        print(assistant_content)

        return assistant_content
    
    


